package etl.log;

import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction2;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.Time;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;

import etl.cmd.HdfsCmd;
import etl.cmd.LoadDataCmd;
import etl.cmd.SaveDataCmd;
import etl.engine.EngineUtil;
import etl.engine.types.DBType;
import etl.engine.types.InputFormatType;
import etl.util.StringUtil;
import scala.Tuple2;

public class StreamLogProcessor {
	
	public static final Logger logger = LogManager.getLogger(StreamLogProcessor.class);
	public static final String LOG_CONSUMER_ID="logconsumerid";
	
	public static void main(String args[]) throws Exception{
//		SparkConf conf = new SparkConf().setAppName("streamLogProcessor").setMaster("local[2]");
		SparkConf conf = new SparkConf().setAppName("streamLogProcessor");
		conf.set("spark.streaming.kafka.consumer.poll.ms", "4096");
		final JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(EngineUtil.getInstance().getLogInterval()));
//		final JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(5L));
		sparkProcess(jsc);
	}
	
	private static String escapeCSV(String v){
		/*
		 *   / --> //
		 *   " --> /"
		 *   ; --> /; 
		 *   
		 *   And enclosed by double-quoted
		 */
		String value="\"" + v.replaceAll("/", "//").replaceAll("\"", "/\"").replaceAll(";", "/;") +"\"";
		return value;
	}
	
	public static void sparkProcess(final JavaStreamingContext jsc){
		try{
			String wfName = "log";
			String defaultFs = EngineUtil.getInstance().getDefaultFs();
			String logTopicName = EngineUtil.getInstance().getLogTopicName();
			
			Map<String, Object> kafkaParams = new HashMap<String, Object>();
			kafkaParams.put("bootstrap.servers", EngineUtil.getInstance().getBootstrapServers());
			kafkaParams.put("group.id", LOG_CONSUMER_ID);
			kafkaParams.put("key.deserializer", StringDeserializer.class);
			kafkaParams.put("value.deserializer", StringDeserializer.class);
			
			logger.info(String.format("kafkaParams:%s", kafkaParams));
			JavaInputDStream<ConsumerRecord<Object, Object>> ds = KafkaUtils.createDirectStream(jsc, 
					LocationStrategies.PreferConsistent(), 
					ConsumerStrategies.Subscribe(Arrays.asList(new String[]{logTopicName}), kafkaParams));
			
			JavaPairDStream<String,String> csvs = ds.mapToPair(new PairFunction<ConsumerRecord<Object,Object>,String,String>(){
				private static final long serialVersionUID = 1L;

				@Override
				public Tuple2<String, String> call(ConsumerRecord<Object, Object> record) throws Exception {
					logger.info(String.format("Log message get: key: %s, value:%s", record.key(), record.value()));
					if (record.key()==null){
						//In legacy version 0.4.0 or older, message without key generated by logger.error() or KafkaMsgGenCmd with no key specified
						//[2017-05-22] it support AUDIT and ERROR log which marker is "AUDIT" or "ERROR" as different against to previous version
						if(record.value()==null) return null;
						String recordValue=record.value().toString();
						if(recordValue.startsWith("VERSION=1")){
							/*
							 * VERSION=1 log4j format: VERSION=1,%marker,%d{yyyy-MM-dd HH:mm:ss.SSS}{UTC},%level,%payload
							 * AUDIT payload format: startdt, enddt, wfname, wfid, actionname, cnt1, cnt2, cnt3, cnt4
							 * ERROR payload format: exception message  or  wfname, wfid, actionname, exception message
							 */
							String[] fields=recordValue.split(",", 5);
							String marker=fields[1];
							String timestamp=fields[2];
							String level=fields[3];
							String payload=fields[4];
							
							
							if("AUDIT".equals(marker)){//AUDIT Log
								/*
								 * Output:
								 * 	Key: etlstat
								 * 	Value:startdt, enddt, wfname, wfid, actionname, cnt1, cnt2, cnt3, cnt4
								 * 
								 * The BDAP application must output enclosed fields as above
								 */
								return new Tuple2<String, String>(LogType.etlstat.toString(), payload+";");
							}else if("ERROR".equals(marker)){//ERROR Log
								/*
								 * Output:
								 * Key: etlexception
								 * Value: startdt, wfname, wfid, actionname, exception message
								 */
								
								fields=payload.split(",",4);
								String wfname=fields[0];
								String wfid=fields[1];
								String actionName=fields[2];
								String message=fields[3];
								
								return new Tuple2<String, String>(LogType.etlexception.toString(), String.format("%s,%s,%s,%s,%s;", timestamp,wfname,wfid,actionName,escapeCSV(message)));
							}else{//Other Exception Log (no marker)
								/*
								 * Output:
								 * Key: etlexception
								 * Value: startdt, wfname, wfid, actionname, exception message
								 */
								
								String message=payload;
								return new Tuple2<String, String>(LogType.etlexception.toString(),  String.format("%s,%s,%s,%s,%s;", timestamp,"","","",escapeCSV(message)));
							}
							
						}else{
							//Legacy Version: r0.2.0/r0.4.0
							ETLLog etllog = new ETLLog(escapeCSV(recordValue));
							return new Tuple2<String, String>(LogType.etlexception.toString(),  etllog.toString()+";");
							
						}
					}else{
						// To support Legacy version LoadDataCmd in r0.2.0/r0.4.0
						return new Tuple2<String, String>(record.key().toString(), record.value().toString()+";");
					}
				}				
			});
			
			
			csvs.cache().foreachRDD(new VoidFunction2<JavaPairRDD<String,String>, Time>(){
				private static final long serialVersionUID = 1L;
				@Override
				public void call(JavaPairRDD<String,String> v1, Time v2) throws Exception {
					if (!v1.isEmpty() && v1.count()>0){
						String batchid = StringUtil.replaceSpaces(v2.toString());
						
						SaveDataCmd saveDataCmd = new SaveDataCmd(wfName, batchid, null, defaultFs, null);
						saveDataCmd.setSendLog(false);
						saveDataCmd.sparkProcessKeyValue(v1, jsc.sparkContext(), InputFormatType.Text, null);
						
						LoadDataCmd loadDataCmd = new LoadDataCmd(wfName, batchid, null, "log", defaultFs, null);
						//if logs are imported to db, import and then delete the files
						if (loadDataCmd.getDbtype()!=DBType.NONE){
							loadDataCmd.setSendLog(false);
							loadDataCmd.sgProcess();
							
							HdfsCmd hdfsCmd = new HdfsCmd(wfName, batchid, null, defaultFs, null);
							String folderName = saveDataCmd.getLogTmpDir();
							
							logger.info(String.format("Clean folder:%s", folderName));
							hdfsCmd.setRmFolders(new String[]{folderName});
							hdfsCmd.sgProcess();
						}
					}
				}
			});
			
			jsc.start();
			jsc.awaitTermination();
			jsc.close();
		}catch(Exception e){
			logger.error("", e);
		}
	}
}
